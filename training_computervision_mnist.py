# -*- coding: utf-8 -*-
"""computervision_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tH0Ic6qdzFrD6Dp9QGSH7_TTbOUPM-Cc

## Coding problem : classifying handwritten digits (MNIST dataset)

The objective of this task is to train a simple neural network on the mnist dataset in order to classify the handwritten digits into numbers ranging from zero to 9.
The problem is a multi class classification problem on image data.

Number of classes = 10

Task = multi class classification

### Reading the data
"""

from keras.datasets import mnist
from matplotlib import pyplot
import tensorflow as tf
from tensorflow import keras
import numpy as np

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score
import cv2
(train_X, train_y), (test_X, test_y) = mnist.load_data()

"""### Data visualisation """
def viz_data():
  '''Visualisation of the shape of the data and a sample of the images'''
  print('X_train:' + str(train_X.shape))
  print('Y_train:' + str(train_y.shape))
  print('X_test:'  + str(test_X.shape))
  print('Y_test:'  + str(test_y.shape))



  for i in range(59995,60000):  
      pyplot.imshow(train_X[i])
      pyplot.show()

"""### Preprocessing"""

def images_preprocessing(train_X,test_X,train_y,test_y):
  '''preprocessing of the MNIST dataset by normalising the values of the dataset and one hot encoding of the lables in order to prepare it for the NN training.'''
  image_size = train_X.shape[1]
  input_size = image_size * image_size


  """1- Resize the NN input fot training purposes"""

  train_X = np.reshape(train_X, [-1, input_size])
  test_X = np.reshape(test_X, [-1, input_size])

  """2- In order to train a neural network the data should be converted into floats in the training set.
  The values of the numbers in the training set should be scaled between 0 and 255 (the data is already standarized) we divide by 255 to normalise the data and have values between 0 and 1.
  """

  train_X=tf.cast(train_X, tf.float32) / 255.0

  train_y=tf.cast(train_y, tf.int32)

  """3- One hot encoding of the classes (y), the hyperparameter depth corresponds to the number of classes"""

  train_y = tf.one_hot(train_y, depth=10)



  """4- Similarily to the previous steps we build a test set scaled and compatible with the tensorflow api."""

  test_X=tf.cast(test_X, tf.float32) / 255.0
  test_y=tf.cast(test_y, tf.int32) 
  test_y = tf.one_hot(test_y, depth=10) 

  '''5- create validation sets'''
  val_X=train_X[55000:]
  train_X=train_X[:55000]
  val_y=train_y[55000:]
  train_y=train_y[:55000]
  return (val_X,val_y)
 

 


 

def train(val_X,val_y):
  '''Results = Accuracy : 98.3 % and 0.06 loss , close to SOA'''
  early_stopping_cb=keras.callbacks.EarlyStopping(patience=10)
  early_stopping_mc=keras.callbacks.ModelCheckpoint("minst.h5",save_best_only=False)

  image_size = train_X.shape[1]
  input_size = image_size * image_size
  '''1- define network architecture by : (i) starting with one layer and a small number of neurons and increasing gradually until reaching good performace. 
  (ii) optimising : applying diffrent values for dropout , regularisation, optimisers and learning rates. 
  (iii) Compiling and building the model.
  (iv) And finally training the model and evaluating it on unseen data (testset)'''
  model = tf.keras.models.Sequential([
      keras.layers.Dense(units=256, activation='relu', input_dim=input_size
                  ),
      keras.layers.Dropout(0.3),
      keras.layers.Dense(units=192, activation='relu'), 
      keras.layers.Dropout(0.3),
      keras.layers.Dense(units=128, activation='relu'),
      keras.layers.Dropout(0.3),
      keras.layers.Dense(units=10, activation='softmax') #number of classes 10  
    ])
  model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy'],
    )
  model.build()
  model.summary()
  '''we use early stopping callbacks to avoid overfitting'''
  model.fit(train_X, train_y, batch_size=500, verbose=1,
                                      validation_data=(val_X, val_y), epochs=90, callbacks=[
          
          early_stopping_cb,
          early_stopping_mc
      ]) 
    
  acc = model.evaluate(test_X, test_y,batch_size=500,)

  return (model,acc)


def validate_model(model):
  """

  ### Model validation

  - accuracy of the best model on the test set is 98,3%



  - the confusion matrix is useful for validating classification models
  """

  best_model=model
  pred_y=best_model.predict(test_X)
  pred_y=np.argmax(pred_y, axis=1)
  test_y=np.argmax(test_y, axis=1)
  cm = confusion_matrix(test_y, pred_y)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm)
  disp.plot()
  pyplot.show()
  '''classification report shows detailed metrics that help validate the model'''
  print(
      f"Classification report for classifier {model}:\n"
      f"{metrics.classification_report(test_y, pred_y)}\n"
  )

import cv2
def predict(model):
  """### Prediction 

  1. Read one image
  2. preprocessing , scaling and normalisation
  3. prediction
  """
  image_size = train_X.shape[1]
  input_size = image_size * image_size

  image_file_path = r"../sample.png"
  image = cv2.imread(image_file_path, cv2.IMREAD_ANYDEPTH)
  pyplot.imshow(image, cmap=pyplot.get_cmap('gray'))
  pyplot.show()

  

  image=np.reshape(image, [-1, input_size])

  Im_pred = tf.cast(image, tf.float32) / 255.0

  Im_pred_y=model.predict(Im_pred)

  Im_pred_y=np.argmax(Im_pred_y, axis=1)

  print(Im_pred_y[0])

  model.save("model")
 

if __name__ == '__main__':
  viz_data()
  (val_X,val_y)= images_preprocessing(train_X,test_X,train_y,test_y)
  model=train(val_X,val_y)
  validate_model(model)
  predict(model)
